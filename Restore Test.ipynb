{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'HtmltoolKit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-96f114529363>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moperator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitemgetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhtmltoolkit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHtmltoolKit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'HtmltoolKit'"
     ]
    }
   ],
   "source": [
    "import cv2, glob, os.path\n",
    "import tensorflow as tf\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter \n",
    "from htmltoolkit \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def optimistic_restore(session, save_file, graph=tf.get_default_graph(), blacklist = []):\n",
    "    reader = tf.train.NewCheckpointReader(save_file)\n",
    "    saved_shapes = reader.get_variable_to_shape_map()\n",
    "    var_names = sorted([(var.name, var.name.split(':')[0]) for var in tf.global_variables()\n",
    "            if var.name.split(':')[0] in saved_shapes])    \n",
    "    restore_vars = []\n",
    "    #skip_names = []\n",
    "    #restore_names = []\n",
    "    for var_name, saved_var_name in var_names:   \n",
    "        valid = True\n",
    "        for patterns in blacklist:\n",
    "            #skip_names.append(var_name) \n",
    "            valid = valid and saved_var_name.find(patterns) == -1\n",
    "            \n",
    "        if valid:\n",
    "            curr_var = graph.get_tensor_by_name(var_name)\n",
    "            var_shape = curr_var.get_shape().as_list()\n",
    "            if var_shape == saved_shapes[saved_var_name]:\n",
    "                #restore_names.append( saved_var_name)\n",
    "                restore_vars.append(curr_var)\n",
    "            else:\n",
    "                pass\n",
    "                #skip_names.append(saved_var_name) \n",
    "    #print('Skipped:\\n',skip_names)\n",
    "    #print('Restored: \\n',restore_names)\n",
    "    #print('Restored vars:\\n',restore_vars)\n",
    "    opt_saver = tf.train.Saver(restore_vars, allow_empty=True)\n",
    "    opt_saver.restore(session, save_file)\n",
    "    \n",
    "\n",
    "\n",
    "dataset_path = './dataset/new-traffic-sign/'\n",
    "htk = HtmlToolkit()\n",
    "js,jQuery = htk.js, htk.jQuery\n",
    "\n",
    "X_new = []\n",
    "y_new = []\n",
    "for path in glob.glob(dataset_path+'*.jpg'):\n",
    "    X_new.append(cv2.imread(path,cv2.IMREAD_COLOR))\n",
    "    # exctract the label number from the full path '<some-dir>/<img-num>_<label>.jpg' \n",
    "    y_new.append(int(os.path.basename(path).split('.')[0].split('_')[1])) \n",
    "n_new = len(X_new)\n",
    "\n",
    "\n",
    "rows = [[{'content':'', 'attr':{'colspan':2}},'Label','Desc','Match','Predictions']]\n",
    "for idx in range(n_new):\n",
    "    num = str(idx)\n",
    "    img = X_new[idx]\n",
    "    label = y_new[idx]\n",
    "    label_num = str(label)\n",
    "    text = class_texts[label]\n",
    "    cols = [\n",
    "        num,\n",
    "        htk.embedImageArray(img,{'id':'img_'+num}),\n",
    "        htk.buildTag('div',{'id':'label_'+num},label_num),\n",
    "        htk.buildTag('div',{'id':'text_'+num},text),\n",
    "        htk.buildTag('div',{'id':'match_'+num},'-'),\n",
    "        htk.buildTag('div',{'id':'pred_'+num},'-')\n",
    "    ]\n",
    "    rows.append(cols)\n",
    "\n",
    "htk.printHTML(htk.htmlTable(rows,{'id':'pred_table'}))\n",
    "X_new = [processImage(image) for image in X_new]\n",
    "X_new = np.array(X_new)\n",
    "\n",
    "basedir = './checkpoints/'\n",
    "restore_name ='20170326-090427' #load old model \n",
    "model_name_load = basedir + restore_name+'/model-'+restore_name\n",
    "check_name_load = basedir + restore_name+'/'\n",
    "info = loadModelInfo(basedir,restore_name)\n",
    "hyperparams = info['hyperparams']\n",
    "#print(hyperparams)\n",
    "\n",
    "blacklist = ['Adam','_power'];\n",
    "\n",
    "with tf.variable_scope(\"lallo\",reuse=True):\n",
    "    with tf.Session() as sess:\n",
    "            #load weight & baias from the disk\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "        x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "        y = tf.placeholder(tf.int32, (None))\n",
    "        one_hot_y = tf.one_hot(y, n_classes)\n",
    "\n",
    "        logits, modelParams, modelInfo = BuildNet(x,hyperparams)\n",
    "        model = tf.nn.softmax(logits)\n",
    "        #cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=one_hot_y)\n",
    "\n",
    "        feed_dict = {}\n",
    "        for layerName in modelInfo:\n",
    "            layerInfo = modelInfo[layerName]\n",
    "            for name in layerInfo['placeholders']:\n",
    "                if name.endswith('_keep_prob'):\n",
    "                    plc = layerInfo['placeholders'][name]\n",
    "                    layerParam = modelParams[layerName]\n",
    "                    feed_dict[plc] = 1.0\n",
    "                else:\n",
    "                    print('Trovato placeholder non gestito :(',name)\n",
    "\n",
    "\n",
    "\n",
    "        optimistic_restore(sess, model_name_load,blacklist=blacklist)\n",
    "\n",
    "        #t = modelInfo['0_conv']['variables']['0_conv_W']\n",
    "        #print(t.name)#,t.eval())\n",
    "        #tens = tf.get_default_graph().get_tensor_by_name('0_conv_W')\n",
    "        #print(tens.eval())\n",
    "\n",
    "        max_bar_length = 100\n",
    "        predictions = {}\n",
    "        num_pred = 5\n",
    "        print(len(X_new))\n",
    "        for i in range(n_new):\n",
    "            num = str(i)\n",
    "            feed_dict.update({ x: X_new[i:i+1], y: y_new[i:i+1] })\n",
    "            #feed_dict.update({ x: X_new[0:1], y: y_new[0:1] })\n",
    "            prob = sess.run(model, feed_dict=feed_dict)[0]\n",
    "            prob_list = []\n",
    "            for jk in range(len(prob)):\n",
    "                prob_list.append({\n",
    "                    'prob':prob[jk],\n",
    "                    'label': jk\n",
    "                });\n",
    "            #print(prob_list)\n",
    "            prob_list_sorted = sorted(prob_list, key=lambda x: x['prob'], reverse=True)\n",
    "            #print(prob_list_sorted)\n",
    "\n",
    "            top5 = prob_list_sorted[:num_pred]\n",
    "            top5rows = []\n",
    "            for item in top5:\n",
    "                prob,label = item['prob'],item['label']\n",
    "                text = class_texts[label]\n",
    "                #print(prob,label, text)\n",
    "                top5rows.append([\n",
    "                    htk.buildTag('div', { 'class':'prob_bar', 'style':'width: {:.0f}%'.format(prob*max_bar_length) } ) +\n",
    "                    htk.buildTag('div', {'class':'prob_float'}, \"{:.0f}%\".format(prob*100) ),\n",
    "                    str(label),\n",
    "                    text,\n",
    "                ])\n",
    "            table = htk.htmlTable(top5rows, {'id':'pred_table_'+num})\n",
    "            #print(table)\n",
    "            print(num)\n",
    "            jQuery('#pred_'+num).html(table)\n",
    "print(\"mmm\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modeldir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bac5868efd1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_saver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodeldir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnew_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodeldir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vars'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'modeldir' is not defined"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "new_saver = tf.train.import_meta_graph(modeldir)\n",
    "new_saver.restore(sess, tf.train.latest_checkpoint(modeldir))\n",
    "all_vars = tf.get_collection('vars')\n",
    "for v in all_vars:\n",
    "    v_ = sess.run(v)\n",
    "    print(v_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
